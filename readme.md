# ğŸš€ AP Morgan Data 

## ğŸŒŸ Overview
The **AP Morgan Data** focuses on handling, validating, and processing data generated daily by internal applications. This data is initially stored in **Azure Data Lake Storage (ADLS)** and undergoes **schema validation** before being processed and stored in **Delta tables** within Databricks for downstream consumption.

## âœ¨ Features
- ğŸ“… **Daily Data Processing:** Automatically processes data sent to the ADLS location.
- âœ… **Schema Validation:** Ensures data adheres to predefined schema standards using SQL Database.
- ğŸ› ï¸ **Delta Lake Integration:** Stores validated data in Delta tables for analytics and downstream usage.
- ğŸ”’ **Secure Operations:** Leverages Azure Key Vault for managing sensitive credentials securely.

## ğŸ› ï¸ Architecture Workflow

1. **ğŸ“¥ Data Ingestion:**  
   Internal applications send data to ADLS in CSV format daily.

2. **ğŸ” Schema Validation:**  
   Azure Data Factory (ADF) pipelines validate the schema by querying a SQL Database.  
   Invalid data is sent to the "Rejected" folder for further review.

3. **ğŸ—‚ï¸ Data Processing in Databricks:**  
   Validated data is processed and stored in Delta tables.

4. **ğŸ“Š Downstream Usage:**  
   The processed data in Delta tables is available for **Data Analytics (DA)** and **Data Science (DS)** teams.

## ğŸ§° Technologies Used
- ğŸ—„ï¸ **Azure Data Lake Storage (ADLS):** Centralized storage for raw, rejected, and processed data.
- ğŸ”„ **Azure Data Factory (ADF):** Orchestration tool for data movement and schema validation.
- ğŸ” **Azure Key Vault:** Securely manages access credentials for services.
- ğŸ§ª **Azure Databricks:** Processes validated data and writes to Delta tables.
- ğŸ“œ **SQL Database:** Validation script for incoming data schema.

## ğŸ“‚ Repository Structure
```plaintext
AP-Morgan-Data-Project/
|â€” DataBricks/          # Databricks notebooks for processing
|â€” pipeline/            # All pipelines used in this project creation
|â€” Datasets/            # Contains the sample data
|â€” MorganDataADF/       # Contains all global parameters and ARM templates used in this project
|â€” linkedService/       # Linked service created in ADF
|â€” trigger/             # Triggers used for pipeline
|â€” README.md            # Project overview (this file)


## ğŸš€ How to Use

1. **ğŸ”‘ Setup Credentials:**  
   Store necessary credentials (e.g., ADLS, Databricks, SQL) in **Azure Key Vault**.

2. **ğŸ”„ Run ADF Pipelines:**  
   Execute ADF pipelines to validate and process incoming data.

3. **ğŸ§¹ Validate Data:**  
   Ensure that incoming CSV data matches the expected schema.

4. **ğŸ“Š Process Data:**  
   Use the provided Databricks notebooks to process validated data and store it in Delta tables.

## ğŸŒŸ Future Enhancements
- ğŸ¤– Automate schema updates for evolving data structures.
- âš¡ Add real-time data processing capabilities.
- ğŸ“ˆ Improve monitoring and alerting for pipeline failures.

## ğŸ“¬ Contact
For questions or support, reach out to the repository owner at [vignanpatchigolla5@gmail.com].
